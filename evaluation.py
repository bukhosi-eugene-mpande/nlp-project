# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iccUfhVGs9to-tX3gBXWpDVsSzMTsVXM
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U datasets
# %pip install transformers
# %pip install torch
# %pip install sacrebleu
# %pip install tqdm
# %pip install numpy
# %pip install regex
# %pip install accelerate
# %pip install bert-score
# %pip install sentence-transformers
# %pip install unbabel-comet
# %pip install sacremoses
# %pip install huggingface_hub

from datasets import load_dataset
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from tqdm import tqdm
import logging
from sacrebleu import corpus_bleu
import os
import json
from sacrebleu.metrics import BLEU, CHRF
from bert_score import score as bert_score
from sentence_transformers import SentenceTransformer
import numpy as np
from collections import Counter
import re
import requests
from datasets import Dataset
from comet import download_model, load_from_checkpoint
import zipfile
import os
from datetime import datetime
import shutil
import os
from huggingface_hub import login
login(token="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")

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA (NVIDIA GPU) for acceleration")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS (Metal Performance Shaders) for acceleration")
else:
    device = torch.device("cpu")
    print("No GPU acceleration available, using CPU")

def fetch_data_from_github(url):
    response = requests.get(url)
    if response.status_code == 200:
        return [line.strip() for line in response.text.splitlines()]
    else:
        print(f"Failed to fetch data from {url}")
        return []

# folders to store translations

os.makedirs("translations", exist_ok=True)

os.makedirs("translations/nllb-200-distilled-600M", exist_ok=True)

os.makedirs("translations/m2m100_418M", exist_ok=True)

os.makedirs("translations/nllb-200-distilled-1.3B", exist_ok=True)

os.makedirs("translations/opus-mt-en-nso", exist_ok=True)

os.makedirs("translations/opus-mt-en-ha", exist_ok=True)

os.makedirs("translations/m2m100_1.2B", exist_ok=True)

# folders to store refrences for flores 101

os.makedirs("flores101", exist_ok=True)

os.makedirs("flores101/english-sources", exist_ok=True)

os.makedirs("flores101/northern-sotho", exist_ok=True)

os.makedirs("flores101/hausa", exist_ok=True)

os.makedirs("flores101/zulu", exist_ok=True)

# folders to store refrences flores 200

os.makedirs("flores200", exist_ok=True)

os.makedirs("flores200/english-sources", exist_ok=True)

os.makedirs("flores200/northern-sotho", exist_ok=True)

os.makedirs("flores200/hausa", exist_ok=True)

os.makedirs("flores200/zulu", exist_ok=True)

# folders to store refrences for the fix for africa

os.makedirs("floresplus", exist_ok=True)

os.makedirs("floresplus/english-sources", exist_ok=True)

os.makedirs("floresplus/northern-sotho", exist_ok=True)

os.makedirs("floresplus/hausa", exist_ok=True)

os.makedirs("floresplus/zulu", exist_ok=True)

# folders to store refrences for the fix for africa

os.makedirs("floresfixforafrica", exist_ok=True)

os.makedirs("floresfixforafrica/english-sources", exist_ok=True)

os.makedirs("floresfixforafrica/northern-sotho", exist_ok=True)

os.makedirs("floresfixforafrica/hausa", exist_ok=True)

os.makedirs("floresfixforafrica/zulu", exist_ok=True)

#creating metrics folders
os.makedirs("metrics", exist_ok=True)

#metrics folders for nllb
os.makedirs("metrics/nllb-200-distilled-600M", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-600M/flores101", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-600M/flores200", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-600M/floresplus", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-600M/floresfixforafrica", exist_ok=True)

#metrics folders for m2m
os.makedirs("metrics/m2m100_418M", exist_ok=True)

os.makedirs("metrics/m2m100_418M/flores101", exist_ok=True)

os.makedirs("metrics/m2m100_418M/flores200", exist_ok=True)

os.makedirs("metrics/m2m100_418M/floresplus", exist_ok=True)

os.makedirs("metrics/m2m100_418M/floresfixforafrica", exist_ok=True)

#metrics folders for m2m
os.makedirs("metrics/m2m100_1.2B", exist_ok=True)

os.makedirs("metrics/m2m100_1.2B/flores101", exist_ok=True)

os.makedirs("metrics/m2m100_1.2B/flores200", exist_ok=True)

os.makedirs("metrics/m2m100_1.2B/floresplus", exist_ok=True)

os.makedirs("metrics/m2m100_1.2B/floresfixforafrica", exist_ok=True)

#metrics folders for nllb-200-distilled-1.3B
os.makedirs("metrics/nllb-200-distilled-1.3B", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-1.3B/flores101", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-1.3B/flores200", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-1.3B/floresplus", exist_ok=True)

os.makedirs("metrics/nllb-200-distilled-1.3B/floresfixforafrica", exist_ok=True)

#metrics folders for opus-mt-en-nso
os.makedirs("metrics/opus-mt-en-nso", exist_ok=True)

os.makedirs("metrics/opus-mt-en-nso/flores101", exist_ok=True)

os.makedirs("metrics/opus-mt-en-nso/flores200", exist_ok=True)

os.makedirs("metrics/opus-mt-en-nso/floresplus", exist_ok=True)

os.makedirs("metrics/opus-mt-en-nso/floresfixforafrica", exist_ok=True)

#metrics folders for opus-mt-en-ha
os.makedirs("metrics/opus-mt-en-ha", exist_ok=True)

os.makedirs("metrics/opus-mt-en-ha/flores101", exist_ok=True)

os.makedirs("metrics/opus-mt-en-ha/flores200", exist_ok=True)

os.makedirs("metrics/opus-mt-en-ha/floresplus", exist_ok=True)

os.makedirs("metrics/opus-mt-en-ha/floresfixforafrica", exist_ok=True)

#english
eng_dataset_test = load_dataset("gsarti/flores_101", name="eng", split="devtest", cache_dir=None)
eng_dataset_training = load_dataset("gsarti/flores_101", name="eng", split="dev", cache_dir=None)

#nothern sotho
nso_dataset_flores_101_test = load_dataset("gsarti/flores_101", name="nso", split="devtest", cache_dir=None)
nso_dataset_flores_101_training = load_dataset("gsarti/flores_101", name="nso", split="dev", cache_dir=None)

#hausa
hau_dataset_flores_101_test = load_dataset("gsarti/flores_101", name="hau", split="devtest", cache_dir=None)
hau_dataset_flores_101_training = load_dataset("gsarti/flores_101", name="hau", split="dev", cache_dir=None)

#zulu
zul_dataset_flores_101_test = load_dataset("gsarti/flores_101", name="zul", split="devtest", cache_dir=None)
zul_dataset_flores_101_training = load_dataset("gsarti/flores_101", name="zul", split="dev", cache_dir=None)

#english
eng_dataset_flores_200_test = load_dataset("Muennighoff/flores200", name="eng_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
eng_dataset_flores_200_training = load_dataset("Muennighoff/flores200", name="eng_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#nothern sotho
nso_dataset_flores_200_test = load_dataset("Muennighoff/flores200", name="nso_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
nso_dataset_flores_200_training = load_dataset("Muennighoff/flores200", name="nso_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#hausa
hau_dataset_flores_200_test = load_dataset("Muennighoff/flores200", name="hau_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
hau_dataset_flores_200_training = load_dataset("Muennighoff/flores200", name="hau_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#zulu
zul_dataset_flores_200_test = load_dataset("Muennighoff/flores200", name="zul_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
zul_dataset_flores_200_training = load_dataset("Muennighoff/flores200", name="zul_Latn", split="dev", trust_remote_code=True, cache_dir=None)

login(token="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")

#english
eng_dataset_flores_plus_test = load_dataset("openlanguagedata/flores_plus", name="eng_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
eng_dataset_flores_plus_training = load_dataset("openlanguagedata/flores_plus", name="eng_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#nothern sotho
nso_dataset_flores_plus_test = load_dataset("openlanguagedata/flores_plus", name="nso_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
nso_dataset_flores_plus_training = load_dataset("openlanguagedata/flores_plus", name="nso_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#hausa
hau_dataset_flores_plus_test = load_dataset("openlanguagedata/flores_plus", name="hau_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
hau_dataset_flores_plus_training = load_dataset("openlanguagedata/flores_plus", name="hau_Latn", split="dev", trust_remote_code=True, cache_dir=None)

#zulu
zul_dataset_flores_plus_test = load_dataset("openlanguagedata/flores_plus", name="zul_Latn", split="devtest", trust_remote_code=True, cache_dir=None)
zul_dataset_flores_plus_training = load_dataset("openlanguagedata/flores_plus", name="zul_Latn", split="dev", trust_remote_code=True, cache_dir=None)

# data links
zul_dev_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/dev/zul_Latn.dev"
zul_devtest_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/devtest/zul_Latn.devtest"

nso_dev_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/dev/nso_Latn.dev"
nso_devtest_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/devtest/nso_Latn.devtest"

hau_dev_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/dev/hau_Latn.dev"
hau_devtest_url = "https://raw.githubusercontent.com/dsfsi/flores-fix-4-africa/main/data/corrected/devtest/hau_Latn.devtest"

#zulu
zul_dataset_flores_fix_for_africa_test = fetch_data_from_github(zul_devtest_url)
zul_dataset_flores_fix_for_africa_training = fetch_data_from_github(zul_dev_url)

#nothern sotho
nso_dataset_flores_fix_for_africa_test = fetch_data_from_github(nso_devtest_url)
nso_dataset_flores_fix_for_africa_training = fetch_data_from_github(nso_dev_url)

#hausa
hau_dataset_flores_fix_for_africa_test = fetch_data_from_github(hau_devtest_url)
hau_dataset_flores_fix_for_africa_training = fetch_data_from_github(hau_dev_url)

assert len(eng_dataset_test) == len(nso_dataset_flores_101_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(nso_dataset_flores_101_training), "Mismatched training dataset sizes"
assert len(eng_dataset_test) == len(zul_dataset_flores_101_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(zul_dataset_flores_101_training), "Mismatched training dataset sizes"
assert len(eng_dataset_test) == len(hau_dataset_flores_101_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(hau_dataset_flores_101_training), "Mismatched training dataset sizes"

assert len(eng_dataset_test) == len(nso_dataset_flores_fix_for_africa_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(nso_dataset_flores_fix_for_africa_training), "Mismatched training dataset sizes"
assert len(eng_dataset_test) == len(zul_dataset_flores_fix_for_africa_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(zul_dataset_flores_fix_for_africa_training), "Mismatched training dataset sizes"
assert len(eng_dataset_test) == len(hau_dataset_flores_fix_for_africa_test), "Mismatched test dataset sizes"
assert len(eng_dataset_training) == len(hau_dataset_flores_fix_for_africa_training), "Mismatched training dataset sizes"

# English Sources for flores 101
with open("flores101/english-sources/flores101.english.source.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_test)

with open("flores101/english-sources/flores101.english.source.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_training)

# English Sources for fix for africa
with open("floresfixforafrica/english-sources/floresfixforafrica.english.source.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_test)

with open("floresfixforafrica/english-sources/floresfixforafrica.english.source.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_training)

# English Sources for flores 200
with open("flores200/english-sources/flores200.english.source.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_flores_200_test)

with open("flores200/english-sources/flores200.english.source.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_flores_200_training)

# English Sources for flores 200
with open("floresplus/english-sources/floresplus.english.source.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_flores_200_test)

with open("floresplus/english-sources/floresplus.english.source.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in eng_dataset_flores_200_training)

# Northern Sotho

with open("floresplus/northern-sotho/floresplus.northern-sotho.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in nso_dataset_flores_plus_test)

with open("floresplus/northern-sotho/floresplus.northern-sotho.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in nso_dataset_flores_plus_training)

# Hausa

with open("floresplus/hausa/floresplus.hausa.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in hau_dataset_flores_plus_test)

with open("floresplus/hausa/floresplus.hausa.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in hau_dataset_flores_plus_training)

# Zulu
with open("floresplus/zulu/floresplus.zulu.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in zul_dataset_flores_plus_test)

with open("floresplus/zulu/floresplus.zulu.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["text"] + "\n" for line in zul_dataset_flores_plus_training)

# Northern Sotho

with open("flores101/northern-sotho/flores101.northern-sotho.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in nso_dataset_flores_101_test)

with open("flores101/northern-sotho/flores101.northern-sotho.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in nso_dataset_flores_101_training)

# Hausa

with open("flores101/hausa/flores101.hausa.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in hau_dataset_flores_101_test)

with open("flores101/hausa/flores101.hausa.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in hau_dataset_flores_101_training)

# Zulu
with open("flores101/zulu/flores101.zulu.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in zul_dataset_flores_101_test)

with open("flores101/zulu/flores101.zulu.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in zul_dataset_flores_101_training)

# Northern Sotho

with open("floresfixforafrica/northern-sotho/floresfixforafrica.northern-sotho.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in nso_dataset_flores_fix_for_africa_test)

with open("floresfixforafrica/northern-sotho/floresfixforafrica.northern-sotho.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in nso_dataset_flores_fix_for_africa_training)

# Hausa

with open("floresfixforafrica/hausa/floresfixforafrica.hausa.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in hau_dataset_flores_fix_for_africa_test)

with open("floresfixforafrica/hausa/floresfixforafrica.hausa.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in hau_dataset_flores_fix_for_africa_training)

# Zulu

with open("floresfixforafrica/zulu/floresfixforafrica.zulu.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in zul_dataset_flores_fix_for_africa_test)

with open("floresfixforafrica/zulu/floresfixforafrica.zulu.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line + "\n" for line in zul_dataset_flores_fix_for_africa_training)

# Northern Sotho

with open("flores200/northern-sotho/flores200.northern-sotho.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in nso_dataset_flores_200_test)

with open("flores200/northern-sotho/flores200.northern-sotho.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in nso_dataset_flores_200_training)

# Hausa

with open("flores200/hausa/flores200.hausa.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in hau_dataset_flores_200_test)

with open("flores200/hausa/flores200.hausa.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in hau_dataset_flores_200_training)

# Zulu

with open("flores200/zulu/flores200.zulu.ref.test.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in zul_dataset_flores_200_test)

with open("flores200/zulu/flores200.zulu.ref.training.txt", "w", encoding="utf-8") as f:
    f.writelines(line["sentence"] + "\n" for line in zul_dataset_flores_200_training)

def load_and_prepare_data(target_lang, split="dev"):
    """Load and prepare the FLORES-101 dataset for the specified language."""
    try:
        dataset_name = "facebook/flores"
        source_lang = "eng_Latn"

        # Map language codes to their full format with script
        lang_code_map = {
            "hau": "hau_Latn",
            "nso": "nso_Latn",
            "zul": "zul_Latn"
        }

        target_lang_full = lang_code_map.get(target_lang)
        if not target_lang_full:
            raise ValueError(f"Unsupported target language code: {target_lang}")

        # Load source and target datasets
        source_dataset = load_dataset(dataset_name, name=source_lang, split=split, trust_remote_code=True)
        target_dataset = load_dataset(dataset_name, name=target_lang_full, split=split, trust_remote_code=True)

        # Create training pairs
        training_data = {
            "input_text": [src["sentence"] for src in source_dataset],
            "target_text": [tgt["sentence"] for tgt in target_dataset]
        }

        # Convert to HuggingFace Dataset
        from datasets import Dataset
        return Dataset.from_dict(training_data)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        raise

def initialize_model(model_path):
    """Initialize the model and tokenizer."""
    try:
        # Check for MPS (Metal Performance Shaders) availability
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print("Using CUDA (NVIDIA GPU) for acceleration")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            print("Using MPS (Metal Performance Shaders) for acceleration")
        else:
            device = torch.device("cpu")
            print("No GPU acceleration available, using CPU")

        # Load model and tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)

        return tokenizer, model, device
    except Exception as e:
        print(f"Error initializing model: {e}")
        raise

def calculate_metrics(hypotheses, references, sources, lang, device):
    """Calculate various translation metrics including COMET.

    Args:
        hypotheses: List of translated texts (machine translations)
        references: List of reference translations (human translations)
        sources: List of source texts (original texts in source language)
        lang: Language code (e.g., 'en', 'de')
        device: Computation device ('cpu' or 'cuda')
    """
    metrics = {}

    try:
        # BLEU score
        bleu = BLEU()
        metrics['BLEU'] = bleu.corpus_score(hypotheses, [references]).score

        # chrF score
        chrf = CHRF()
        metrics['chrF'] = chrf.corpus_score(hypotheses, [references]).score

        # BERTScore
        P, R, F1 = bert_score(hypotheses, references, lang=lang, device=device)
        metrics['BERTScore'] = F1.mean().item()

        # COMET score (masakhane/africomet-mtl)
        comet_model_path = download_model("masakhane/africomet-mtl")
        comet_model = load_from_checkpoint(comet_model_path)

        comet_data = [{'src': src, 'mt': hyp, 'ref': ref}
                     for src, hyp, ref in zip(sources, hypotheses, references)]

        comet_scores = comet_model.predict(
            comet_data,
            batch_size=8,
            gpus=1 if torch.cuda.is_available() else 0
        )

        metrics['COMET'] = comet_scores.system_score

        # # Semantic Similarity Score (optional)
        # model = SentenceTransformer('all-MiniLM-L6-v2')
        # hyp_embeddings = model.encode(hypotheses, convert_to_tensor=True)
        # ref_embeddings = model.encode(references, convert_to_tensor=True)
        # similarity = torch.nn.functional.cosine_similarity(hyp_embeddings, ref_embeddings)
        # metrics['Semantic_Score'] = similarity.mean().item()

        # Error Type Frequency Distribution
        error_types = analyze_errors(hypotheses, references)
        metrics['Error_Distribution'] = error_types

    except Exception as e:
        print(f"Error calculating metrics: {e}")
        metrics['error'] = str(e)

    return metrics

def analyze_errors(hypotheses, references):
    """Analyze translation errors and their distribution."""
    error_types = Counter()

    for hyp, ref in zip(hypotheses, references):
        # Word order errors
        hyp_words = set(hyp.split())
        ref_words = set(ref.split())
        if hyp_words == ref_words and hyp != ref:
            error_types['word_order'] += 1

        # Missing words
        missing = ref_words - hyp_words
        if missing:
            error_types['missing_words'] += len(missing)

        # Extra words
        extra = hyp_words - ref_words
        if extra:
            error_types['extra_words'] += len(extra)

        # Case errors
        if hyp.lower() == ref.lower() and hyp != ref:
            error_types['case_errors'] += 1

        # Punctuation errors
        hyp_no_punct = re.sub(r'[^\w\s]', '', hyp)
        ref_no_punct = re.sub(r'[^\w\s]', '', ref)
        if hyp_no_punct == ref_no_punct and hyp != ref:
            error_types['punctuation_errors'] += 1

    return dict(error_types)

# Function to save metrics to a file
def save_metrics_to_file(metrics, lang, datasetname, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    metrics_file = f"{output_dir}/{datasetname}/{lang}_metrics.json"
    print(metrics_file)

    with open(metrics_file, "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=4)

# Function to load saved metrics from a file
def load_metrics_from_file(lang, datasetname, output_dir):
    metrics_file = f"{output_dir}/{datasetname}/{lang}_metrics.json"
    if os.path.exists(metrics_file):
        with open(metrics_file, "r", encoding="utf-8") as f:
            return json.load(f)
    return None  # Return None if no metrics file exists

def create_dataset_from_sentences(source_dataset, target_dataset):
    """
    Creates a Hugging Face Dataset from source and target sentence pairs.

    Parameters:
    - source_dataset (list of dicts): The source language dataset, e.g., English.
    - target_dataset (list of sentences): The target language dataset, e.g., Hausa.

    Returns:
    - A Hugging Face Dataset object containing 'input_text' and 'target_text'.
    """
    # Ensure the datasets are non-empty and have matching lengths
    if len(source_dataset) != len(target_dataset):
        raise ValueError("Source and target datasets must have the same length.")

    # Create pairs of 'input_text' and 'target_text'
    training_data = {
        "input_text": [src["sentence"] for src in source_dataset],
        "target_text": [tgt for tgt in target_dataset]
    }

    # Convert to Hugging Face Dataset
    return Dataset.from_dict(training_data)

def create_dataset_from_sentences_with_dict(source_dataset, target_dataset):
    """
    Creates a Hugging Face Dataset from source and target sentence pairs.

    Parameters:
    - source_dataset (list of dicts): The source language dataset, e.g., English.
    - target_dataset (list of sentences): The target language dataset, e.g., Hausa.

    Returns:
    - A Hugging Face Dataset object containing 'input_text' and 'target_text'.
    """
    # Ensure the datasets are non-empty and have matching lengths
    if len(source_dataset) != len(target_dataset):
        raise ValueError("Source and target datasets must have the same length.")

    # Create pairs of 'input_text' and 'target_text'
    training_data = {
        "input_text": [src["sentence"] for src in source_dataset],
        "target_text": [tgt["sentence"] for tgt in target_dataset]
    }

    # Convert to Hugging Face Dataset
    return Dataset.from_dict(training_data)

def translate_english_to_target_lang(model, tokenizer, device, ref_sentences, output_file, target_lang_code, lang_codes, batch_size=16):
    """Translate English sentences to target language using the pre trained model."""
    try:

        target_lang = lang_codes.get(target_lang_code)
        if not target_lang:
            raise ValueError(f"Unsupported target language code: {target_lang_code}")

        forced_bos_token_id = tokenizer.convert_tokens_to_ids(target_lang)

        print(f"Starting translation to {target_lang}...")

        # Prepare all sentences for batch processing
        all_sentences = ref_sentences["input_text"]

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            # Process in batches
            for i in tqdm(range(0, len(all_sentences), batch_size), desc=f"Translating to {target_lang}"):
                batch_sentences = all_sentences[i:i + batch_size]

                # Tokenize batch without length limits
                inputs = tokenizer(
                    batch_sentences,
                    return_tensors="pt",
                    padding=True,
                    truncation=False
                ).to(device)

                with torch.no_grad():
                    translated = model.generate(
                        **inputs,
                        forced_bos_token_id=forced_bos_token_id,
                        num_beams=4,
                        early_stopping=True
                    )

                # Decode and write batch results
                decoded = tokenizer.batch_decode(translated, skip_special_tokens=True)
                for translation in decoded:
                    f.write(translation.strip() + "\n")

        print(f"Translation completed. Results saved to {output_file}")
    except Exception as e:
        print(f"Error during translation: {e}")
        raise

from transformers import pipeline, M2M100ForConditionalGeneration, M2M100Tokenizer
import torch
from tqdm import tqdm

def translate_using_m2m(
    eng_dataset,
    output_file,
    target_lang_code,
    device,
    model_name,
    batch_size=16,
    max_length=200  # Added max length parameter
):
    # Load model and tokenizer directly for more control
    tokenizer = M2M100Tokenizer.from_pretrained(model_name, src_lang="en")
    model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)

    # Enhanced generation parameters
    gen_kwargs = {
        "forced_bos_token_id": tokenizer.get_lang_id(target_lang_code),
        "num_beams": 5,
        "early_stopping": True,
        "no_repeat_ngram_size": 2,
        "length_penalty": 2.0,
        "repetition_penalty": 2.5,  # Stronger penalty for repetition
        "max_length": max_length,
        "num_return_sequences": 1,
    }

    with open(output_file, "w", encoding="utf-8") as f:
        for i in tqdm(range(0, len(eng_dataset), batch_size), desc="Translating"):
            batch = eng_dataset[i:i+batch_size]["sentence"]

            # Tokenize with more aggressive truncation
            inputs = tokenizer(
                batch,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=150,  # Shorter input limit
            ).to(device)

            # Generate translations with better control
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    **gen_kwargs
                )

            # Decode with language token filtering
            decoded = tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            for text in decoded:
                f.write(text.strip() + "\n")

def translate_using_opus(
    dataset,
    output_file,
    target_lang_code,
    device,
    batch_size=16,
    max_length=300,
):
    """
    Translate sentences using OPUS-MT models.

    Args:
        dataset: Dataset containing source sentences (either Dataset object or list of dicts)
        output_file: Path to save translations
        target_lang_code: Target language code
        device: Torch device
        batch_size: Batch size for translation
        max_length: Maximum length of translations
    """
    # Create output directory if needed
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    # Initialize model
    model_name = f"Helsinki-NLP/opus-mt-en-{target_lang_code}"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
    except Exception as e:
        print(f"Error loading model {model_name}: {str(e)}")
        return

    # Extract source sentences
    if isinstance(dataset, Dataset):
        source_sentences = dataset["input_text"] if "input_text" in dataset.features else [x["sentence"] for x in dataset]
    elif isinstance(dataset, list):
        source_sentences = [x["sentence"] for x in dataset]
    else:
        raise ValueError("Unsupported dataset format")

    # Translate and save
    with open(output_file, 'w', encoding='utf-8') as f_out:
        for i in tqdm(range(0, len(source_sentences), batch_size), desc="Translating"):
            batch = source_sentences[i:i+batch_size]

            inputs = tokenizer(
                batch,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=max_length
            ).to(device)

            outputs = model.generate(**inputs, max_length=max_length)
            translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)

            for trans in translations:
                f_out.write(trans.strip() + '\n')

    print(f"Saved translations to {output_file}")

#translation by nllb for each language

# Configuration
languages = {
    "hausa": "hau",
    "northern-sotho": "nso",
    "zulu": "zul"
}

models = [
    {
        "nllb-200-distilled-600M" : "facebook/nllb-200-distilled-600M",
        "lang_codes" : {
            "hau": "hau_Latn",
            "nso": "nso_Latn",
            "zul": "zul_Latn"
        }
    },
    {
        "nllb-200-distilled-1.3B" : "facebook/nllb-200-distilled-1.3B",
        "lang_codes" : {
            "hau": "hau_Latn",
            "nso": "nso_Latn",
            "zul": "zul_Latn"
        }
    },
    {
        "m2m100_418M" : "facebook/m2m100_418M",
        "lang_codes" : {
            "hau": "ha",
            "nso": "ns",
            "zul": "zu"
        }
    },
    {
        "m2m100_1.2B" : "facebook/m2m100_1.2B",
        "lang_codes" : {
            "hau": "ha",
            "nso": "ns",
            "zul": "zu"
        }
    },
    {
        "opus-mt-en-nso" : "Helsinki-NLP/opus-mt-en-nso",
        "lang_codes" : {
            "nso": "nso",
        }
    },
    {
        "opus-mt-en-ha" : "Helsinki-NLP/opus-mt-en-ha",
        "lang_codes" : {
            "hau": "ha",
        }
    }
]

translations_base_dir = "translations"

for model in models:
    model_name = list(model.keys())[0]  # Get the model name

    # Check if the model path exists before attempting to use it
    if model_name not in model:
        print(f"Error: No path found for model {model_name}")
        continue

    model_path = model[model_name]      # Get the Hugging Face path

    # Validate model_path is not None
    if not model_path:
        print(f"Error: Path for model {model_name} is None or empty")
        continue

    lang_codes = model["lang_codes"]    # Get the language codes dict

    print(f"\n Translating using the Model: {model_name} ({model_path})")

    # Initialize model and tokenizer
    try:
        tokenizer, model, device = initialize_model(model_path)
    except Exception as e:
        print(f"Error initializing model {model_name}: {e}")
        continue

# Translate for each language
for lang, code in languages.items():
      translations_file = f"{translations_base_dir}/{model_name}/{lang}.hyp.txt"

      # Check if language code exists in model's supported languages
      if code not in lang_codes:
          print(f"Warning: Language code '{code}' for {lang} not found in {model_name}'s supported languages. Skipping...")
          continue

      # Check if translation already exists
      if os.path.exists(translations_file) and os.path.getsize(translations_file) > 0:
          print(f"Translation for {lang} already exists. Skipping translation...")
      else:
          print(f"Translating to {lang}...")
          if(lang == "hausa"):
            # handling huasa translations
            if(model_name == "opus-mt-en-ha"):
              test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, hau_dataset_flores_101_test)
              translate_using_opus(test_data,translations_file,lang_codes[code],device)
            elif(model_name == "m2m100_418M" or model_name == "m2m100_1.2B"):
              test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, hau_dataset_flores_101_test)
              translate_using_m2m(eng_dataset_test,translations_file,lang_codes[code],device,model_path)
            else:
              test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, hau_dataset_flores_101_test)
              translate_english_to_target_lang(model, tokenizer, device, test_data, translations_file, code, lang_codes)

          elif(lang == "zulu"):
              # handling nothern zulu translations
              if(model_name == "m2m100_418M" or model_name == "m2m100_1.2B"):
                test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, zul_dataset_flores_101_test)
                translate_using_m2m(eng_dataset_test,translations_file,lang_codes[code],device,model_path)
              else:
                test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, zul_dataset_flores_101_test)
                translate_english_to_target_lang(model, tokenizer, device, test_data, translations_file, code, lang_codes)

          elif(lang == "northern-sotho"):
            # handling nothern sotho translations
            if(model_name == "Helsinki-NLP/opus-mt-en-nso"):
              test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, nso_dataset_flores_101_test)
              translate_using_opus(test_data,translations_file,lang_codes[code],device)
            elif(model_name == "m2m100_418M" or model_name == "m2m100_1.2B"):
                test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, nso_dataset_flores_101_test)
                translate_using_m2m(eng_dataset_test,translations_file,lang_codes[code],device,model_path)
            else:
              test_data = create_dataset_from_sentences_with_dict(eng_dataset_test, nso_dataset_flores_101_test)
              translate_english_to_target_lang(model, tokenizer, device, test_data, translations_file, code, lang_codes)

flores_datasets = [
    "flores101",
    "flores200",
    "floresplus",
    "floresfixforafrica",
]

for model in models:
    model_name = list(model.keys())[0]  # Get the model name
    model_path = model[model_name]      # Get the Hugging Face path
    lang_codes = model["lang_codes"]    # Get the language codes dict

    for flores_dataset in flores_datasets:
        # Calculate metric for each language
        for lang, code in languages.items():

            # Check if language is supported by this model
            if code not in lang_codes:
                print(f"Warning: Language code '{code}' for {lang} not found in {model_name}'s supported languages. Skipping...")
                continue

            translations_file = f"{translations_base_dir}/{model_name}/{lang}.hyp.txt"

            # Check if metrics are already saved
            metrics = load_metrics_from_file(lang, flores_dataset, f"metrics/{model_name}")

            if not metrics:
                # Calculate metrics
                try:
                    with open(translations_file, "r", encoding="utf-8") as f:
                        hyps = [line.strip() for line in f]

                    with open(f"{flores_dataset}/{lang}/{flores_dataset}.{lang}.ref.test.txt", "r", encoding="utf-8") as f:
                        refs = [line.strip() for line in f]

                    with open(f"{flores_dataset}/english-sources/{flores_dataset}.english.source.test.txt", "r", encoding="utf-8") as f:
                        sources = [line.strip() for line in f]

                    metrics = calculate_metrics(hyps, refs, sources, lang, device)

                    # Save metrics to file
                    save_metrics_to_file(metrics, lang, flores_dataset, f"metrics/{model_name}")

                    print(f"\nMetrics for {lang} using {flores_dataset} dataset and {model_name} for translation:")
                    print("-" * 30)
                    for metric_name, value in metrics.items():
                        if metric_name != 'Error_Distribution':
                            print(f"{metric_name:15}: {value:.4f}")

                    if 'Error_Distribution' in metrics:
                        print("\nError Distribution:")
                        for error_type, count in metrics['Error_Distribution'].items():
                            print(f"{error_type:20}: {count}")

                except Exception as e:
                    print(f"Error calculating metrics for {lang}: {e}")
                    print(f"Could not calculate metrics for {lang}")

        # Print summary of all metrics
        print(f"\nSummary of all metrics for {flores_dataset} using {model_name} for translation:")
        print("-" * 60)
        print(f"{'Language':15} {'BLEU':>8} {'chrF':>8} {'BERTScore':>10} {'COMET':>8}")
        print("-" * 60)

        for lang, code in languages.items():
            try:
                # Check if language is supported by this model
                if code not in lang_codes:
                    print(f"{lang:15} {'--':>8} {'--':>8} {'--':>10} {'--':>8}")
                    continue

                # Load metrics from file
                metrics = load_metrics_from_file(lang, flores_dataset, f"metrics/{model_name}")

                if metrics:
                    print(f"{lang:15} {float(metrics['BLEU']):8.2f} {float(metrics['chrF']):8.2f} {float(metrics['BERTScore']):10.2f} {float(metrics['COMET']):8.2f}")
                else:
                    print(f"{lang:15} {'Error':>8} {'Error':>8} {'Error':>10} {'Error':>8}")

            except Exception as e:
                print(f"Error loading metrics for {lang}: {e}")
                print(f"{lang:15} {'Error':>8} {'Error':>8} {'Error':>10} {'Error':>8}")

        print("-" * 60)

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.gridspec import GridSpec

def load_all_metrics(metrics_base_dir='metrics'):
    """
    Load all metrics from the metrics directory structure.

    Returns:
        pandas.DataFrame: DataFrame containing all metrics data
    """
    data = []

    # Walk through the metrics directory
    for model_dir in os.listdir(metrics_base_dir):
        model_path = os.path.join(metrics_base_dir, model_dir)
        if not os.path.isdir(model_path):
            continue

        for dataset_dir in os.listdir(model_path):
            dataset_path = os.path.join(model_path, dataset_dir)
            if not os.path.isdir(dataset_path):
                continue

            for metrics_file in os.listdir(dataset_path):
                if not metrics_file.endswith('_metrics.json'):
                    continue

                language = metrics_file.split('_')[0]
                file_path = os.path.join(dataset_path, metrics_file)

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        metrics = json.load(f)

                    # Extract the main metrics
                    row = {
                        'model': model_dir,
                        'dataset': dataset_dir,
                        'language': language,
                        'BLEU': float(metrics.get('BLEU', 0)),
                        'chrF': float(metrics.get('chrF', 0)),
                        'BERTScore': float(metrics.get('BERTScore', 0)),
                        'COMET': float(metrics.get('COMET', 0))
                    }

                    # Add error distribution if available
                    if 'Error_Distribution' in metrics:
                        for error_type, count in metrics['Error_Distribution'].items():
                            row[f'error_{error_type}'] = count

                    data.append(row)
                except Exception as e:
                    print(f"Error loading metrics from {file_path}: {e}")

    return pd.DataFrame(data)

def create_model_comparison_chart(df, metric='BLEU', output_dir='visualizations'):
    """
    Create a bar chart comparing models across languages for a specific metric.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        metric (str): Metric to visualize (e.g., 'BLEU', 'COMET')
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    plt.figure(figsize=(12, 8))

    # Create grouped bar chart
    ax = sns.barplot(x='language', y=metric, hue='model', data=df, palette='viridis')

    # Enhance the plot
    plt.title(f'Model Comparison: {metric} Scores by Language', fontsize=16)
    plt.xlabel('Language', fontsize=14)
    plt.ylabel(f'{metric} Score', fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.legend(title='Model', fontsize=12, title_fontsize=13)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    # Add value labels on the bars
    for container in ax.containers:
        ax.bar_label(container, fmt='%.2f', fontsize=10)

    # Adjust layout and save
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f'{metric}_model_comparison.png'), dpi=300, bbox_inches='tight')
    plt.close()

def create_dataset_specific_heatmaps(df, output_dir='visualizations/dataset_heatmaps'):
    """
    Create separate heatmaps for each dataset showing model vs language performance across metrics.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    # List of metrics to visualize
    metrics = ['BLEU', 'chrF', 'BERTScore', 'COMET']

    # For each dataset, create a separate heatmap
    for dataset in df['dataset'].unique():
        dataset_df = df[df['dataset'] == dataset]

        # Create a heatmap for each metric
        for metric in metrics:
            # Pivot the data to create model vs language matrix
            pivot_df = dataset_df.pivot_table(
                index='model',
                columns='language',
                values=metric
            ).fillna(0)

            # Create figure with appropriate size
            plt.figure(figsize=(10, 8))

            # Set appropriate colormap based on metric
            cmap = 'viridis'
            if metric == 'COMET':
                # For COMET, use diverging colormap as it can have negative values
                cmap = 'RdYlGn'

            # Create heatmap
            ax = sns.heatmap(
                pivot_df,
                annot=True,
                cmap=cmap,
                fmt='.2f',
                linewidths=.5
            )

            # Set titles and labels
            plt.title(f'{metric} Scores - {dataset} Dataset', fontsize=16)
            plt.ylabel('Model', fontsize=14)
            plt.xlabel('Language', fontsize=14)

            # Adjust layout and save
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, f'{dataset}_{metric}_heatmap.png'), dpi=300, bbox_inches='tight')
            plt.close()

        # Create heatmaps for error distributions if available
        error_columns = [col for col in df.columns if col.startswith('error_')]

        if error_columns:
            for error_col in error_columns:
                error_type = error_col.replace('error_', '')

                # Pivot the data
                pivot_df = dataset_df.pivot_table(
                    index='model',
                    columns='language',
                    values=error_col
                ).fillna(0)

                plt.figure(figsize=(10, 8))
                ax = sns.heatmap(
                    pivot_df,
                    annot=True,
                    cmap='YlOrRd',
                    fmt='.1f',
                    linewidths=.5
                )

                plt.title(f'{error_type.replace("_", " ").title()} Errors - {dataset} Dataset', fontsize=16)
                plt.ylabel('Model', fontsize=14)
                plt.xlabel('Language', fontsize=14)

                plt.tight_layout()
                plt.savefig(os.path.join(output_dir, f'{dataset}_error_{error_type}_heatmap.png'), dpi=300, bbox_inches='tight')
                plt.close()

def create_language_comparison_chart(df, model, output_dir='visualizations'):
    """
    Create a radar chart comparing metrics across languages for a specific model.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        model (str): Model to visualize
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    # Filter data for the specific model
    model_df = df[df['model'] == model]

    # Get unique languages
    languages = model_df['language'].unique()

    # Metrics to visualize (excluding error counts)
    metrics = ['BLEU', 'chrF', 'BERTScore', 'COMET']

    # Create radar chart
    plt.figure(figsize=(10, 8))
    ax = plt.subplot(111, polar=True)

    # Number of metrics
    N = len(metrics)

    # Angles for each metric (evenly spaced)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # Close the loop

    # Color palette
    colors = sns.color_palette("husl", len(languages))

    # Plot each language
    for i, language in enumerate(languages):
        lang_df = model_df[model_df['language'] == language]

        # Get values for each metric
        values = [lang_df[metric].values[0] for metric in metrics]

        # Normalize values to 0-1 scale for better visualization
        # This assumes BLEU, chrF are 0-100 and BERTScore, COMET are -1 to 1
        normalized_values = []
        for j, val in enumerate(values):
            if metrics[j] in ['BLEU', 'chrF']:
                normalized_values.append(val / 100)
            else:
                normalized_values.append((val + 1) / 2)

        # Close the loop
        normalized_values += normalized_values[:1]

        # Plot
        ax.plot(angles, normalized_values, linewidth=2, linestyle='solid', color=colors[i], label=language)
        ax.fill(angles, normalized_values, color=colors[i], alpha=0.1)

    # Set labels
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(metrics, fontsize=12)

    # Remove radial labels and set y-ticks
    ax.set_yticklabels([])
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])

    # Add gridlines
    ax.grid(True)

    # Add legend
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)

    plt.title(f'Metrics Comparison Across Languages: {model}', fontsize=16, y=1.1)
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir, f'{model}_language_comparison_radar.png'), dpi=300, bbox_inches='tight')
    plt.close()

def create_error_distribution_chart(df, output_dir='visualizations'):
    """
    Create a heatmap showing error distribution across languages and models.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    # Filter columns that contain error information
    error_columns = [col for col in df.columns if col.startswith('error_')]

    if not error_columns:
        print("No error distribution data found")
        return

    # Create a separate figure for each error type
    for error_col in error_columns:
        error_type = error_col.replace('error_', '')

        # Reshape data for heatmap
        pivot_df = df.pivot_table(
            index='language',
            columns='model',
            values=error_col,
            aggfunc='mean'
        ).fillna(0)

        plt.figure(figsize=(10, 7))
        sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', fmt='.1f', linewidths=.5)

        plt.title(f'Distribution of {error_type.replace("_", " ").title()} Errors', fontsize=16)
        plt.tight_layout()

        plt.savefig(os.path.join(output_dir, f'error_{error_type}_heatmap.png'), dpi=300, bbox_inches='tight')
        plt.close()

def create_dataset_comparison_chart(df, output_dir='visualizations'):
    """
    Create bar charts comparing datasets for each model and metric.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    # Metrics to visualize
    metrics = ['BLEU', 'chrF', 'BERTScore', 'COMET']

    # Get unique models
    models = df['model'].unique()

    for model in models:
        model_df = df[df['model'] == model]

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        for i, metric in enumerate(metrics):
            # Create grouped bar chart
            sns.barplot(
                x='language',
                y=metric,
                hue='dataset',
                data=model_df,
                palette='Set2',
                ax=axes[i]
            )

            # Enhance the subplot
            axes[i].set_title(f'{metric} Scores by Dataset', fontsize=14)
            axes[i].set_xlabel('Language', fontsize=12)
            axes[i].set_ylabel(f'{metric} Score', fontsize=12)
            axes[i].grid(axis='y', linestyle='--', alpha=0.7)

            # Add value labels
            for container in axes[i].containers:
                axes[i].bar_label(container, fmt='%.2f', fontsize=9)

        # Set overall title
        plt.suptitle(f'Dataset Comparison for {model}', fontsize=18)

        # Adjust layout and save
        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle
        plt.savefig(os.path.join(output_dir, f'{model}_dataset_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()

def create_comprehensive_dashboard(df, output_dir='visualizations'):
    """
    Create a comprehensive dashboard with all metrics and comparisons.

    Args:
        df (pandas.DataFrame): DataFrame with metrics data
        output_dir (str): Directory to save visualizations
    """
    os.makedirs(output_dir, exist_ok=True)

    # Create a large figure for the dashboard
    plt.figure(figsize=(24, 16))

    # Define grid layout
    gs = GridSpec(3, 4, figure=plt.gcf())

    # 1. Overall BLEU scores comparison
    ax1 = plt.subplot(gs[0, :2])
    sns.barplot(x='language', y='BLEU', hue='model', data=df, ax=ax1)
    ax1.set_title('BLEU Scores by Model and Language', fontsize=14)
    ax1.set_xlabel('Language', fontsize=12)
    ax1.set_ylabel('BLEU Score', fontsize=12)
    ax1.legend(title='Model', fontsize=10)

    # 2. Overall COMET scores comparison
    ax2 = plt.subplot(gs[0, 2:])
    sns.barplot(x='language', y='COMET', hue='model', data=df, ax=ax2)
    ax2.set_title('COMET Scores by Model and Language', fontsize=14)
    ax2.set_xlabel('Language', fontsize=12)
    ax2.set_ylabel('COMET Score', fontsize=12)
    ax2.legend(title='Model', fontsize=10)

    # 3. Performance across metrics (boxplot)
    ax3 = plt.subplot(gs[1, :2])
    metrics_df = df[['BLEU', 'chrF', 'BERTScore', 'COMET']].copy()
    # Normalize metrics for better comparison
    metrics_df['BLEU'] = metrics_df['BLEU'] / 100
    metrics_df['chrF'] = metrics_df['chrF'] / 100
    metrics_df_melted = pd.melt(metrics_df)
    sns.boxplot(x='variable', y='value', data=metrics_df_melted, ax=ax3)
    ax3.set_title('Distribution of Normalized Metric Scores', fontsize=14)
    ax3.set_xlabel('Metric', fontsize=12)
    ax3.set_ylabel('Normalized Score (0-1)', fontsize=12)

    # 4. Dataset comparison
    ax4 = plt.subplot(gs[1, 2:])
    sns.barplot(x='dataset', y='BLEU', hue='model', data=df, ax=ax4)
    ax4.set_title('BLEU Scores by Dataset and Model', fontsize=14)
    ax4.set_xlabel('Dataset', fontsize=12)
    ax4.set_ylabel('BLEU Score', fontsize=12)
    ax4.legend(title='Model', fontsize=10)

    # 5. Error analysis
    ax5 = plt.subplot(gs[2, :2])
    error_cols = [col for col in df.columns if col.startswith('error_')]
    if error_cols:
        error_df = df[['model', 'language'] + error_cols].melt(
            id_vars=['model', 'language'],
            var_name='error_type',
            value_name='count'
        )
        error_df['error_type'] = error_df['error_type'].str.replace('error_', '')
        sns.barplot(x='error_type', y='count', hue='model', data=error_df, ax=ax5)
        ax5.set_title('Error Distribution by Type and Model', fontsize=14)
        ax5.set_xlabel('Error Type', fontsize=12)
        ax5.set_ylabel('Count', fontsize=12)
        ax5.legend(title='Model', fontsize=10)
        plt.setp(ax5.get_xticklabels(), rotation=45, ha='right')

    # 6. Correlation heatmap between metrics
    ax6 = plt.subplot(gs[2, 2:])
    corr = df[['BLEU', 'chrF', 'BERTScore', 'COMET']].corr()
    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=ax6)
    ax6.set_title('Correlation Between Metrics', fontsize=14)

    plt.suptitle('Translation Models Evaluation Dashboard', fontsize=20)
    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for suptitle

    plt.savefig(os.path.join(output_dir, 'evaluation_dashboard.png'), dpi=300, bbox_inches='tight')
    plt.close()

def generate_all_visualizations():
    """Generate all visualization types for the translation metrics data."""
    # Create visualizations directory
    os.makedirs('visualizations', exist_ok=True)

    # Load all metrics data
    print("Loading metrics data...")
    df = load_all_metrics()

    if df.empty:
        print("No metrics data found. Please run evaluation first.")
        return

    print(f"Loaded data for {len(df)} metric entries.")

    # Generate visualizations
    print("Generating model comparison charts...")
    for metric in ['BLEU', 'chrF', 'BERTScore', 'COMET']:
        create_model_comparison_chart(df, metric)

    print("Generating language comparison charts...")
    for model in df['model'].unique():
        create_language_comparison_chart(df, model)

    print("Generating error distribution charts...")
    create_error_distribution_chart(df)

    print("Generating dataset comparison charts...")
    create_dataset_comparison_chart(df)

    print("Generating dataset-specific heatmaps...")
    create_dataset_specific_heatmaps(df)

    print("Generating comprehensive dashboard...")
    create_comprehensive_dashboard(df)

    print("All visualizations generated in the 'visualizations' directory.")


# Generate visualizations
print("\nGenerating visualizations for metric comparisons...")
try:
    generate_all_visualizations()
    print("Visualizations generated successfully in the 'visualizations' directory.")
except Exception as e:
    print(f"Error generating visualizations: {e}")

def zip_folders():
    # Create a timestamp for the zip filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    zip_filename = f"translation_results_{timestamp}.zip"

    # List of folders to zip
    folders_to_zip = [
        "translations",
        "flores101",
        "flores200",
        "floresplus",
        "floresfixforafrica",
        "metrics",
        "visualizations"
    ]

    # Create a zip file
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for folder in folders_to_zip:
            # Walk through each folder and add files to the zip
            for root, dirs, files in os.walk(folder):
                for file in files:
                    file_path = os.path.join(root, file)
                    # Add file to zip (maintains directory structure)
                    zipf.write(file_path, os.path.relpath(file_path, start=os.path.dirname(folder)))

    print(f"Successfully created zip archive: {zip_filename}")
    return zip_filename

# Run the function
zip_path = zip_folders()
